{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\udit1\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('winequality-red.csv')\n",
    "\n",
    "data['quality'] = 1*data['quality'].apply(lambda x: True if(x>6) else False,)\n",
    "\n",
    "#min max scaling\n",
    "#X = X-Xmin/Xmax-Xmin\n",
    "l = (data.drop('quality',axis=1)).columns\n",
    "for i in l:\n",
    "    data[i] = data[i].apply(lambda x: (x-min(data[i]))/(max(data[i])-min(data[i])))\n",
    "data.to_csv('Logistic_regression.csv')\n",
    "# MAKING THE DATA SET FOR ID3 ALGORITHM\n",
    "data_ = pd.read_csv('winequality-red.csv')\n",
    "for i in range(len(data_['quality'])):\n",
    "    if data_['quality'].iloc[i] < 5:\n",
    "        data_['quality'].iloc[i]=0\n",
    "    elif ((data_['quality'].iloc[i] == 5) or (data_['quality'].iloc[i] == 6)):\n",
    "        data_['quality'].iloc[i] = 1\n",
    "    else:\n",
    "        data_['quality'].iloc[i]=2\n",
    "#NORMALIZING THE DATA \n",
    "for i in l:\n",
    "    data_[i]=data_[i].apply(lambda x: (x-data_[i].mean())/np.std(data_[i]))\n",
    "#DIVIDING DATA INTO BINS 0,1,2,3\n",
    "for i in l:\n",
    "    bin_size = (max(data_[i]) - min(data_[i]))/4\n",
    "    start = min(data_[i])\n",
    "    b_1 = start + bin_size\n",
    "    b_2 = b_1+bin_size\n",
    "    b_3 = b_2+bin_size\n",
    "    end = max(data_[i])\n",
    "    for j in range(len(data_[i])):\n",
    "        if data_[i].iloc[j]>=start and data_[i].iloc[j]<b_1:\n",
    "            data_[i].iloc[j] = 0\n",
    "        elif data_[i].iloc[j]>=b_1 and data_[i].iloc[j]<b_2:\n",
    "            data_[i].iloc[j] = 1\n",
    "        elif data_[i].iloc[j]>=b_2 and data_[i].iloc[j]<b_3:\n",
    "            data_[i].iloc[j] = 2\n",
    "        else:\n",
    "            data_[i].iloc[j] = 3\n",
    "data_.to_csv('ID3_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing Logistic Regression\n",
      "Starting Cost theta initiated as one: 3.388591156716365\n",
      "The metrics for logistic regression using my implementation of logistic regression::\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93      1382\n",
      "           1       0.64      0.32      0.43       217\n",
      "\n",
      "    accuracy                           0.88      1599\n",
      "   macro avg       0.77      0.65      0.68      1599\n",
      "weighted avg       0.87      0.88      0.87      1599\n",
      "\n",
      "The metrics for logistic regression using SAGA solver::\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.94      1382\n",
      "           1       0.64      0.35      0.45       217\n",
      "\n",
      "    accuracy                           0.88      1599\n",
      "   macro avg       0.77      0.66      0.69      1599\n",
      "weighted avg       0.87      0.88      0.87      1599\n",
      "\n",
      "The prediction mismatch between my model and the SAGA solver model is :: 0.0150093808630394\n",
      "\n",
      "\n",
      "Implementing Cross-Validation\n",
      "\n",
      "\n",
      "Starting Cost theta initiated as one: 3.3332080658694396\n",
      "Starting Cost theta initiated as one: 3.4245553184026996\n",
      "Starting Cost theta initiated as one: 3.4080100858769566\n",
      "\n",
      "\n",
      "The metrics for a 3 fold cross validation on my logistic regression model\n",
      "precision: 0.5678632478632478\n",
      "recall: 0.303051462130746\n",
      "accuracy: 0.8736710444027517\n",
      "The metrics for a 3 fold cross validation on SAGA solver model\n",
      "\n",
      "\n",
      "precision: 0.5512121212121213\n",
      "recall: 0.3094241501775748\n",
      "accuracy: 0.8736801229697626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\udit1\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\udit1\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\udit1\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#implementing logistic regression\n",
    "\n",
    "thresh_hold = 0.0000001#Adjust this for faster processing !\n",
    "def Gradient_Descent(X_train,y,theta):\n",
    "\tlin_est = np.dot(np.transpose(theta),X_train) ## computing here the h(x)\n",
    "\tt_1 = np.dot(np.transpose(y),np.log(sigmoid(lin_est))) ##computing y*log(h(x))\n",
    "\tt_2 = np.dot(np.transpose(1-y),np.log(1-sigmoid(lin_est)))##computing (1-y)*h(x)\n",
    "\terr = -(t_1+t_2) \n",
    "\tcost= err/np.shape(X_train)[1] ##the error term\n",
    "\tprint('Starting Cost theta initiated as one:',cost)\n",
    "\told = cost\n",
    "\tdiff = cost\n",
    "\twhile(diff >= thresh_hold):\n",
    "\t\tgrad = np.dot(X_train,(sigmoid(lin_est)-y))\n",
    "\t\tgrad = grad/np.shape(X_train)[1]\n",
    "\t\talpha = 0.09\n",
    "\t\ttheta = theta - alpha*grad\n",
    "\t\tlin_est = np.dot(np.transpose(theta),X_train)\n",
    "\t\tt_1 = np.dot(np.transpose(y),np.log(sigmoid(lin_est)))\n",
    "\t\tt_2 = np.dot(np.transpose(1-y),np.log(1-sigmoid(lin_est)))\n",
    "\t\terr = -(t_1+t_2)\n",
    "\t\tcost= err/np.shape(X_train)[1]\n",
    "#\t\tprint('Iter_Cost: ',cost)\n",
    "\t\tdiff = abs(cost-old)\n",
    "\t\told = cost\n",
    "\treturn(theta,cost,lin_est)\n",
    "\n",
    "def sigmoid(x):\n",
    "    sigmoid = 1/(1+np.exp(-x))\n",
    "    return(sigmoid)\n",
    "\n",
    "def binary_func(pred_og):\n",
    "    for i in range(len(pred_og)):\n",
    "        if pred_og[i]>0.5:\n",
    "            pred_og[i]=1\n",
    "        else:\n",
    "            pred_og[i]=0\n",
    "    return(pred_og)\n",
    "data = pd.read_csv('Logistic_regression.csv')\n",
    "data.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "print('Implementing Logistic Regression')\n",
    "l = (data.drop('quality',axis=1)).columns\n",
    "feature_list = l \n",
    "feature = (data[feature_list].values) \n",
    "theta = np.ones(len(feature_list)+1) #INITIATING THETA\n",
    "a=np.ones(len(feature)) #ADDING X(0)\n",
    "X_train = np.insert(feature,0,a,axis=1) #ADDING X(0)\n",
    "X_train =np.transpose(X_train)\n",
    "y = np.array(data['quality'])\n",
    "theta,cost,lin_est = Gradient_Descent(X_train,y,theta)\n",
    "pred_og = sigmoid(lin_est)\n",
    "pred_og = binary_func(pred_og)\n",
    "print('The metrics for logistic regression using my implementation of logistic regression::')\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y,pred_og))\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='saga',penalty='none')\n",
    "lr.fit(data.drop('quality',axis=1),data['quality'])\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = lr.predict(data.drop('quality',axis=1))\n",
    "print('The metrics for logistic regression using SAGA solver::')\n",
    "print(classification_report(y,pred))\n",
    "\n",
    "\n",
    "preds = pd.DataFrame(pred_og,columns=['predicted_by_me'])\n",
    "preds['predicted_by_not_me'] = pred\n",
    "error_rate = (preds['predicted_by_me'] != preds['predicted_by_not_me']).mean()\n",
    "\n",
    "print('The prediction mismatch between my model and the SAGA solver model is ::',error_rate)\n",
    "print('\\n')\n",
    "print('Implementing Cross-Validation')\n",
    "print('\\n')\n",
    "\n",
    "##3 fold cross validation for my model begins::\n",
    "\n",
    "slot_1 = data.iloc[0:533]\n",
    "slot_2 = data.iloc[533:1066]\n",
    "slot_3 = data.iloc[1066:1599]\n",
    "\n",
    "precision=[]\n",
    "recall=[]\n",
    "accuracy=[]\n",
    "for i in range(1,4):\n",
    "    if i == 1:\n",
    "        test_set = slot_1\n",
    "        train_set = slot_2.append(slot_3)\n",
    "        feature_list = l #USED LIST OF FEATURES GETS INCREASED EVERY ITERATION\n",
    "        feature = (train_set[feature_list].values) #GETTING VALUES FROM DATA FRAME AS AN ARRAY\n",
    "        theta = np.ones(len(feature_list)+1) #INITIATING THETA\n",
    "        a=np.ones(len(feature)) #ADDING X(0)\n",
    "        X_train = np.insert(feature,0,a,axis=1) #ADDING X(0)\n",
    "        X_train =np.transpose(X_train)\n",
    "        y = np.array(train_set['quality'])\n",
    "        params,cost,lin_est = Gradient_Descent(X_train,y,theta)\n",
    "        feature_test = (test_set[feature_list].values)\n",
    "        theta = params #PARAMS ARE THE FINAL ITERATION PARAMETERS\n",
    "        a=np.ones(len(feature_test)) #ADDING X(0)\n",
    "        X_test = np.insert(feature_test,0,a,axis=1)\n",
    "        X_test =np.transpose(X_test)\n",
    "        y_test = np.array(test_set['quality'])\n",
    "        test_err = np.dot(np.transpose(theta),X_test)\n",
    "        temp = binary_func(sigmoid(test_err))\n",
    "        from sklearn.metrics import classification_report \n",
    "        X =classification_report(y_test,temp,output_dict=True)\n",
    "        precision.append(X['1']['precision'])\n",
    "        recall.append(X['1']['recall'])\n",
    "        accuracy.append(X['accuracy'])\n",
    "    \n",
    "        \n",
    "            \n",
    "    if i == 2:\n",
    "        test_set = slot_2\n",
    "        train_set = slot_1.append(slot_3)\n",
    "        feature_list = l #USED LIST OF FEATURES GETS INCREASED EVERY ITERATION\n",
    "        feature = (train_set[feature_list].values) #GETTING VALUES FROM DATA FRAME AS AN ARRAY\n",
    "        theta = np.ones(len(feature_list)+1) #INITIATING THETA\n",
    "        a=np.ones(len(feature)) #ADDING X(0)\n",
    "        X_train = np.insert(feature,0,a,axis=1) #ADDING X(0)\n",
    "        X_train =np.transpose(X_train)\n",
    "        y = np.array(train_set['quality'])\n",
    "        params,cost,lin_est = Gradient_Descent(X_train,y,theta)\n",
    "        feature_test = (test_set[feature_list].values)\n",
    "        theta = params #PARAMS ARE THE FINAL ITERATION PARAMETERS\n",
    "        a=np.ones(len(feature_test)) #ADDING X(0)\n",
    "        X_test = np.insert(feature_test,0,a,axis=1)\n",
    "        X_test =np.transpose(X_test)\n",
    "        y_test = np.array(test_set['quality'])\n",
    "        test_err = np.dot(np.transpose(theta),X_test)\n",
    "        temp = binary_func(sigmoid(test_err))\n",
    "        from sklearn.metrics import classification_report \n",
    "        X =classification_report(y_test,temp,output_dict=True)\n",
    "        precision.append(X['1']['precision'])\n",
    "        recall.append(X['1']['recall'])\n",
    "        accuracy.append(X['accuracy'])\n",
    "    if i == 3:\n",
    "        test_set = slot_3\n",
    "        train_set = slot_1.append(slot_2)\n",
    "        feature_list = l #USED LIST OF FEATURES GETS INCREASED EVERY ITERATION\n",
    "        feature = (train_set[feature_list].values) #GETTING VALUES FROM DATA FRAME AS AN ARRAY\n",
    "        theta = np.ones(len(feature_list)+1) #INITIATING THETA\n",
    "        a=np.ones(len(feature)) #ADDING X(0)\n",
    "        X_train = np.insert(feature,0,a,axis=1) #ADDING X(0)\n",
    "        X_train =np.transpose(X_train)\n",
    "        y = np.array(train_set['quality'])\n",
    "        params,cost,lin_est = Gradient_Descent(X_train,y,theta)\n",
    "        feature_test = (test_set[feature_list].values)\n",
    "        theta = params #PARAMS ARE THE FINAL ITERATION PARAMETERS\n",
    "        a=np.ones(len(feature_test)) #ADDING X(0)\n",
    "        X_test = np.insert(feature_test,0,a,axis=1)\n",
    "        X_test =np.transpose(X_test)\n",
    "        y_test = np.array(test_set['quality'])\n",
    "        test_err = np.dot(np.transpose(theta),X_test)\n",
    "        temp = binary_func(sigmoid(test_err))\n",
    "        from sklearn.metrics import classification_report \n",
    "        X =classification_report(y_test,temp,output_dict=True)\n",
    "        precision.append(X['1']['precision'])\n",
    "        recall.append(X['1']['recall'])\n",
    "        accuracy.append(X['accuracy'])\n",
    "print('\\n')\n",
    "print('The metrics for a 3 fold cross validation on my logistic regression model')\n",
    "print('precision:',sum(precision)/3)\n",
    "print('recall:',sum(recall)/3)\n",
    "print('accuracy:',sum(accuracy)/3)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "l = (data.drop('quality',axis=1)).columns\n",
    "precision_saga=cross_val_score(lr,data[l],data['quality'],cv=3,scoring = 'precision')\n",
    "recall_saga=cross_val_score(lr,data[l],data['quality'],cv=3,scoring = 'recall')\n",
    "accuracy_saga=cross_val_score(lr,data[l],data['quality'],cv=3,scoring = 'accuracy')\n",
    "print('The metrics for a 3 fold cross validation on SAGA solver model')\n",
    "print('\\n')\n",
    "print('precision:',sum(precision_saga)/3)\n",
    "print('recall:',sum(recall_saga)/3)\n",
    "print('accuracy:',sum(accuracy_saga)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The metrics for ID3 algorithm are::\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.37      0.46        63\n",
      "           1       0.89      0.97      0.93      1319\n",
      "           2       0.78      0.42      0.55       217\n",
      "\n",
      "    accuracy                           0.87      1599\n",
      "   macro avg       0.77      0.59      0.65      1599\n",
      "weighted avg       0.86      0.87      0.86      1599\n",
      "\n",
      "The metrics for scikit learn implementation with a min_samples_split =10:: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.30      0.43        63\n",
      "           1       0.90      0.97      0.93      1319\n",
      "           2       0.76      0.53      0.62       217\n",
      "\n",
      "    accuracy                           0.88      1599\n",
      "   macro avg       0.80      0.60      0.66      1599\n",
      "weighted avg       0.87      0.88      0.87      1599\n",
      "\n",
      "The metrics for a 3 fold cross validation on Decision tree classifier model\n",
      "\n",
      "\n",
      "Macro precision: 0.4942172223686427\n",
      "Macro recall: 0.4152687951972336\n",
      "accuracy: 0.7986025707860097\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import log2 as log\n",
    "data = pd.read_csv('ID3_dataset.csv')\n",
    "data.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "#calculating entropy\n",
    "def entropy_node(data):\n",
    "    determine = data.keys()[-1]\n",
    "    l = data[determine].unique()\n",
    "    a = data[determine].value_counts()\n",
    "    entropy = 0\n",
    "    for i in l:\n",
    "        prob = a[i]/len(data[determine])\n",
    "        entropy = entropy-prob*log(prob)\n",
    "    return(entropy)\n",
    "def attribute_entropy(data,attribute):\n",
    "    determine = data.keys()[-1]\n",
    "    l = data[attribute].unique()\n",
    "    attribute_entropy = 0\n",
    "    for i in l:\n",
    "        entropy_atr_class = entropy_node(data[data[attribute]==i])\n",
    "        attribute_entropy += entropy_atr_class*len(data[data[attribute]==i][determine])/len(data[determine])\n",
    "    return(attribute_entropy)\n",
    "\n",
    "## finding the maximum gain = entropy_node - attribute entropy\n",
    "def max_gain_class(data):\n",
    "    determine = data.keys()[-1]\n",
    "    attributes = data.keys()[:-1]\n",
    "    info_gain = {}\n",
    "    for i in attributes:\n",
    "        temp = entropy_node(data) - attribute_entropy(data,i)\n",
    "        info_gain[i] = temp\n",
    "    winner_class = max(info_gain,key=info_gain.get)\n",
    "    return(winner_class)\n",
    "\n",
    "def decision_tree(data,tree=None):\n",
    "    determine = data.keys()[-1]\n",
    "\n",
    "    node = max_gain_class(data)\n",
    "    #the components of the max gain class\n",
    "    l = data[node].unique()\n",
    "    if tree is None:\n",
    "        tree = {}\n",
    "        tree[node] ={}\n",
    "    for i in l:\n",
    "        sub_data = data[data[node]==i].reset_index(drop=True)\n",
    "        y = len(sub_data.columns)\n",
    "        if len(sub_data[determine])<=10 or y==2:#here y==2 stops the tree when only one feature is remaining in the feature set, as there will be no features in future iterations\n",
    "            tree[node][i] = sub_data[determine].mode()[0]# assigns the mode of the remaining data to the leaf\n",
    "        else:\n",
    "            tree[node][i]=decision_tree(sub_data.drop(node,axis=1))#here we drop the current node feature from the sub_data and pass it for further split\n",
    "    return(tree)\n",
    "def predict(data,tree):#this is a code to traverse the tree dictionary\n",
    "    temp = tree[list(tree.keys())[0]][data[list(tree.keys())[0]]]\n",
    "    if (temp ==1) or (temp ==0) or (temp ==2):\n",
    "        return(temp)\n",
    "    else:\n",
    "        tree =tree[list(tree.keys())[0]][data[list(tree.keys())[0]]]\n",
    "        temp=predict(data,tree)\n",
    "        return(temp)\n",
    "# getting the tree\n",
    "tree =decision_tree(data)\n",
    "#getting the predictions\n",
    "pred=[]\n",
    "for i in range(len(data)):\n",
    "    pred.append(predict(data.iloc[i],tree))\n",
    "#Checking the performance of ID3 algorithm\n",
    "print('The metrics for ID3 algorithm are::')\n",
    "predictions = pd.DataFrame(pred,columns=['predictions'])\n",
    "predictions['true'] = data['quality']\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predictions['true'],predictions['predictions']))\n",
    "#Computing the Decision Tree Classifier Using Scikitlearn\n",
    "print('The metrics for scikit learn implementation with a min_samples_split =10:: ')\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dr = DecisionTreeClassifier(min_samples_split=10)\n",
    "dr.fit(data.drop('quality',axis=1),data['quality'])\n",
    "pred_=dr.predict(data.drop('quality',axis=1))\n",
    "print(classification_report(predictions['true'],pred_))\n",
    "from sklearn.model_selection import cross_val_score\n",
    "l = (data.drop('quality',axis=1)).columns\n",
    "precision_=cross_val_score(dr,data[l],data['quality'],cv=3,scoring = 'precision_macro')\n",
    "recall_=cross_val_score(dr,data[l],data['quality'],cv=3,scoring = 'recall_macro')\n",
    "accuracy_=cross_val_score(dr,data[l],data['quality'],cv=3,scoring = 'accuracy')\n",
    "print('The metrics for a 3 fold cross validation on Decision tree classifier model')\n",
    "print('\\n')\n",
    "print('Macro precision:',sum(precision_)/3)\n",
    "print('Macro recall:',sum(recall_)/3)\n",
    "print('accuracy:',sum(accuracy_)/3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing Logistic Regression\n",
      "Starting Cost theta initiated as one: 3.388591156716365\n",
      "The metrics for logistic regression using my implementation of logistic regression::\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93      1382\n",
      "           1       0.64      0.32      0.43       217\n",
      "\n",
      "    accuracy                           0.88      1599\n",
      "   macro avg       0.77      0.65      0.68      1599\n",
      "weighted avg       0.87      0.88      0.87      1599\n",
      "\n",
      "The metrics for logistic regression using SAGA solver::\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.94      1382\n",
      "           1       0.64      0.35      0.45       217\n",
      "\n",
      "    accuracy                           0.88      1599\n",
      "   macro avg       0.77      0.66      0.69      1599\n",
      "weighted avg       0.87      0.88      0.87      1599\n",
      "\n",
      "The prediction mismatch between my model and the SAGA solver model is :: 0.0150093808630394\n",
      "\n",
      "\n",
      "Implementing Cross-Validation\n",
      "\n",
      "\n",
      "Starting Cost theta initiated as one: 3.3332080658694396\n",
      "Starting Cost theta initiated as one: 3.4245553184026996\n",
      "Starting Cost theta initiated as one: 3.4080100858769566\n",
      "\n",
      "\n",
      "The metrics for a 3 fold cross validation on my logistic regression model\n",
      "precision: 0.5678632478632478\n",
      "recall: 0.303051462130746\n",
      "accuracy: 0.8736710444027517\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\udit1\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\udit1\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\udit1\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The metrics for a 3 fold cross validation on SAGA solver model\n",
      "precision: 0.5512121212121213\n",
      "recall: 0.3094241501775748\n",
      "accuracy: 0.8736801229697626\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#implementing logistic regression\n",
    "\n",
    "thresh_hold = 0.0000001#Adjust this for faster processing !\n",
    "def Gradient_Descent(X_train,y,theta):\n",
    "\tlin_est = np.dot(np.transpose(theta),X_train) ## computing here the h(x)\n",
    "\tt_1 = np.dot(np.transpose(y),np.log(sigmoid(lin_est))) ##computing y*log(h(x))\n",
    "\tt_2 = np.dot(np.transpose(1-y),np.log(1-sigmoid(lin_est)))##computing (1-y)*h(x)\n",
    "\terr = -(t_1+t_2) \n",
    "\tcost= err/np.shape(X_train)[1] ##the error term\n",
    "\tprint('Starting Cost theta initiated as one:',cost)\n",
    "\told = cost\n",
    "\tdiff = cost\n",
    "\twhile(diff >= thresh_hold):\n",
    "\t\tgrad = np.dot(X_train,(sigmoid(lin_est)-y))\n",
    "\t\tgrad = grad/np.shape(X_train)[1]\n",
    "\t\talpha = 0.09\n",
    "\t\ttheta = theta - alpha*grad\n",
    "\t\tlin_est = np.dot(np.transpose(theta),X_train)\n",
    "\t\tt_1 = np.dot(np.transpose(y),np.log(sigmoid(lin_est)))\n",
    "\t\tt_2 = np.dot(np.transpose(1-y),np.log(1-sigmoid(lin_est)))\n",
    "\t\terr = -(t_1+t_2)\n",
    "\t\tcost= err/np.shape(X_train)[1]\n",
    "#\t\tprint('Iter_Cost: ',cost)\n",
    "\t\tdiff = abs(cost-old)\n",
    "\t\told = cost\n",
    "\treturn(theta,cost,lin_est)\n",
    "\n",
    "def sigmoid(x):\n",
    "    sigmoid = 1/(1+np.exp(-x))\n",
    "    return(sigmoid)\n",
    "\n",
    "def binary_func(pred_og):\n",
    "    for i in range(len(pred_og)):\n",
    "        if pred_og[i]>0.5:\n",
    "            pred_og[i]=1\n",
    "        else:\n",
    "            pred_og[i]=0\n",
    "    return(pred_og)\n",
    "data = pd.read_csv('Logistic_regression.csv')\n",
    "data.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "print('Implementing Logistic Regression')\n",
    "l = (data.drop('quality',axis=1)).columns\n",
    "feature_list = l \n",
    "feature = (data[feature_list].values) \n",
    "theta = np.ones(len(feature_list)+1) #INITIATING THETA\n",
    "a=np.ones(len(feature)) #ADDING X(0)\n",
    "X_train = np.insert(feature,0,a,axis=1) #ADDING X(0)\n",
    "X_train =np.transpose(X_train)\n",
    "y = np.array(data['quality'])\n",
    "theta,cost,lin_est = Gradient_Descent(X_train,y,theta)\n",
    "pred_og = sigmoid(lin_est)\n",
    "pred_og = binary_func(pred_og)\n",
    "print('The metrics for logistic regression using my implementation of logistic regression::')\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y,pred_og))\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='saga',penalty='none')\n",
    "lr.fit(data.drop('quality',axis=1),data['quality'])\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = lr.predict(data.drop('quality',axis=1))\n",
    "print('The metrics for logistic regression using SAGA solver::')\n",
    "print(classification_report(y,pred))\n",
    "\n",
    "\n",
    "preds = pd.DataFrame(pred_og,columns=['predicted_by_me'])\n",
    "preds['predicted_by_not_me'] = pred\n",
    "error_rate = (preds['predicted_by_me'] != preds['predicted_by_not_me']).mean()\n",
    "\n",
    "print('The prediction mismatch between my model and the SAGA solver model is ::',error_rate)\n",
    "print('\\n')\n",
    "print('Implementing Cross-Validation')\n",
    "print('\\n')\n",
    "\n",
    "##3 fold cross validation for my model begins::\n",
    "\n",
    "slot_1 = data.iloc[0:533]\n",
    "slot_2 = data.iloc[533:1066]\n",
    "slot_3 = data.iloc[1066:1599]\n",
    "\n",
    "precision=[]\n",
    "recall=[]\n",
    "accuracy=[]\n",
    "for i in range(1,4):\n",
    "    if i == 1:\n",
    "        test_set = slot_1\n",
    "        train_set = slot_2.append(slot_3)\n",
    "        feature_list = l #USED LIST OF FEATURES GETS INCREASED EVERY ITERATION\n",
    "        feature = (train_set[feature_list].values) #GETTING VALUES FROM DATA FRAME AS AN ARRAY\n",
    "        theta = np.ones(len(feature_list)+1) #INITIATING THETA\n",
    "        a=np.ones(len(feature)) #ADDING X(0)\n",
    "        X_train = np.insert(feature,0,a,axis=1) #ADDING X(0)\n",
    "        X_train =np.transpose(X_train)\n",
    "        y = np.array(train_set['quality'])\n",
    "        params,cost,lin_est = Gradient_Descent(X_train,y,theta)\n",
    "        feature_test = (test_set[feature_list].values)\n",
    "        theta = params #PARAMS ARE THE FINAL ITERATION PARAMETERS\n",
    "        a=np.ones(len(feature_test)) #ADDING X(0)\n",
    "        X_test = np.insert(feature_test,0,a,axis=1)\n",
    "        X_test =np.transpose(X_test)\n",
    "        y_test = np.array(test_set['quality'])\n",
    "        test_err = np.dot(np.transpose(theta),X_test)\n",
    "        temp = binary_func(sigmoid(test_err))\n",
    "        from sklearn.metrics import classification_report \n",
    "        X =classification_report(y_test,temp,output_dict=True)\n",
    "        precision.append(X['1']['precision'])\n",
    "        recall.append(X['1']['recall'])\n",
    "        accuracy.append(X['accuracy'])\n",
    "    \n",
    "        \n",
    "            \n",
    "    if i == 2:\n",
    "        test_set = slot_2\n",
    "        train_set = slot_1.append(slot_3)\n",
    "        feature_list = l #USED LIST OF FEATURES GETS INCREASED EVERY ITERATION\n",
    "        feature = (train_set[feature_list].values) #GETTING VALUES FROM DATA FRAME AS AN ARRAY\n",
    "        theta = np.ones(len(feature_list)+1) #INITIATING THETA\n",
    "        a=np.ones(len(feature)) #ADDING X(0)\n",
    "        X_train = np.insert(feature,0,a,axis=1) #ADDING X(0)\n",
    "        X_train =np.transpose(X_train)\n",
    "        y = np.array(train_set['quality'])\n",
    "        params,cost,lin_est = Gradient_Descent(X_train,y,theta)\n",
    "        feature_test = (test_set[feature_list].values)\n",
    "        theta = params #PARAMS ARE THE FINAL ITERATION PARAMETERS\n",
    "        a=np.ones(len(feature_test)) #ADDING X(0)\n",
    "        X_test = np.insert(feature_test,0,a,axis=1)\n",
    "        X_test =np.transpose(X_test)\n",
    "        y_test = np.array(test_set['quality'])\n",
    "        test_err = np.dot(np.transpose(theta),X_test)\n",
    "        temp = binary_func(sigmoid(test_err))\n",
    "        from sklearn.metrics import classification_report \n",
    "        X =classification_report(y_test,temp,output_dict=True)\n",
    "        precision.append(X['1']['precision'])\n",
    "        recall.append(X['1']['recall'])\n",
    "        accuracy.append(X['accuracy'])\n",
    "    if i == 3:\n",
    "        test_set = slot_3\n",
    "        train_set = slot_1.append(slot_2)\n",
    "        feature_list = l #USED LIST OF FEATURES GETS INCREASED EVERY ITERATION\n",
    "        feature = (train_set[feature_list].values) #GETTING VALUES FROM DATA FRAME AS AN ARRAY\n",
    "        theta = np.ones(len(feature_list)+1) #INITIATING THETA\n",
    "        a=np.ones(len(feature)) #ADDING X(0)\n",
    "        X_train = np.insert(feature,0,a,axis=1) #ADDING X(0)\n",
    "        X_train =np.transpose(X_train)\n",
    "        y = np.array(train_set['quality'])\n",
    "        params,cost,lin_est = Gradient_Descent(X_train,y,theta)\n",
    "        feature_test = (test_set[feature_list].values)\n",
    "        theta = params #PARAMS ARE THE FINAL ITERATION PARAMETERS\n",
    "        a=np.ones(len(feature_test)) #ADDING X(0)\n",
    "        X_test = np.insert(feature_test,0,a,axis=1)\n",
    "        X_test =np.transpose(X_test)\n",
    "        y_test = np.array(test_set['quality'])\n",
    "        test_err = np.dot(np.transpose(theta),X_test)\n",
    "        temp = binary_func(sigmoid(test_err))\n",
    "        from sklearn.metrics import classification_report \n",
    "        X =classification_report(y_test,temp,output_dict=True)\n",
    "        precision.append(X['1']['precision'])\n",
    "        recall.append(X['1']['recall'])\n",
    "        accuracy.append(X['accuracy'])\n",
    "print('\\n')\n",
    "print('The metrics for a 3 fold cross validation on my logistic regression model')\n",
    "print('precision:',sum(precision)/3)\n",
    "print('recall:',sum(recall)/3)\n",
    "print('accuracy:',sum(accuracy)/3)\n",
    "print('\\n')\n",
    "from sklearn.model_selection import cross_val_score\n",
    "l = (data.drop('quality',axis=1)).columns\n",
    "precision_saga=cross_val_score(lr,data[l],data['quality'],cv=3,scoring = 'precision')\n",
    "recall_saga=cross_val_score(lr,data[l],data['quality'],cv=3,scoring = 'recall')\n",
    "accuracy_saga=cross_val_score(lr,data[l],data['quality'],cv=3,scoring = 'accuracy')\n",
    "print('The metrics for a 3 fold cross validation on SAGA solver model')\n",
    "print('precision:',sum(precision_saga)/3)\n",
    "print('recall:',sum(recall_saga)/3)\n",
    "print('accuracy:',sum(accuracy_saga)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The metrics for ID3 algorithm are::\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.37      0.46        63\n",
      "           1       0.89      0.97      0.93      1319\n",
      "           2       0.78      0.42      0.55       217\n",
      "\n",
      "    accuracy                           0.87      1599\n",
      "   macro avg       0.77      0.59      0.65      1599\n",
      "weighted avg       0.86      0.87      0.86      1599\n",
      "\n",
      "The metrics for scikit learn implementation with a min_samples_split =10:: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.30      0.43        63\n",
      "           1       0.90      0.97      0.93      1319\n",
      "           2       0.77      0.51      0.61       217\n",
      "\n",
      "    accuracy                           0.88      1599\n",
      "   macro avg       0.80      0.59      0.66      1599\n",
      "weighted avg       0.87      0.88      0.87      1599\n",
      "\n",
      "The metrics for a 3 fold cross validation on Decision tree classifier model\n",
      "\n",
      "\n",
      "Macro precision: 0.514211608285181\n",
      "Macro recall: 0.42185048511225687\n",
      "accuracy: 0.801729529535273\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import log2 as log\n",
    "data = pd.read_csv('ID3_dataset.csv')\n",
    "data.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "#calculating entropy\n",
    "def entropy_node(data):\n",
    "    determine = data.keys()[-1]\n",
    "    l = data[determine].unique()\n",
    "    a = data[determine].value_counts()\n",
    "    entropy = 0\n",
    "    for i in l:\n",
    "        prob = a[i]/len(data[determine])\n",
    "        entropy = entropy-prob*log(prob)\n",
    "    return(entropy)\n",
    "def attribute_entropy(data,attribute):\n",
    "    determine = data.keys()[-1]\n",
    "    l = data[attribute].unique()\n",
    "    attribute_entropy = 0\n",
    "    for i in l:\n",
    "        entropy_atr_class = entropy_node(data[data[attribute]==i])\n",
    "        attribute_entropy += entropy_atr_class*len(data[data[attribute]==i][determine])/len(data[determine])\n",
    "    return(attribute_entropy)\n",
    "\n",
    "## finding the maximum gain = entropy_node - attribute entropy\n",
    "def max_gain_class(data):\n",
    "    determine = data.keys()[-1]\n",
    "    attributes = data.keys()[:-1]\n",
    "    info_gain = {}\n",
    "    for i in attributes:\n",
    "        temp = entropy_node(data) - attribute_entropy(data,i)\n",
    "        info_gain[i] = temp\n",
    "    winner_class = max(info_gain,key=info_gain.get)\n",
    "    return(winner_class)\n",
    "\n",
    "def decision_tree(data,tree=None):\n",
    "    determine = data.keys()[-1]\n",
    "\n",
    "    node = max_gain_class(data)\n",
    "    #the components of the max gain class\n",
    "    l = data[node].unique()\n",
    "    if tree is None:\n",
    "        tree = {}\n",
    "        tree[node] ={}\n",
    "    for i in l:\n",
    "        sub_data = data[data[node]==i].reset_index(drop=True)\n",
    "        y = len(sub_data.columns)\n",
    "        if len(sub_data[determine])<=10 or y==2:#here y==2 stops the tree when only one feature is remaining in the feature set, as there will be no features in future iterations\n",
    "            tree[node][i] = sub_data[determine].mode()[0]# assigns the mode of the remaining data to the leaf\n",
    "        else:\n",
    "            tree[node][i]=decision_tree(sub_data.drop(node,axis=1))#here we drop the current node feature from the sub_data and pass it for further split\n",
    "    return(tree)\n",
    "def predict(data,tree):#this is a code to traverse the tree dictionary\n",
    "    temp = tree[list(tree.keys())[0]][data[list(tree.keys())[0]]]\n",
    "    if (temp ==1) or (temp ==0) or (temp ==2):\n",
    "        return(temp)\n",
    "    else:\n",
    "        tree =tree[list(tree.keys())[0]][data[list(tree.keys())[0]]]\n",
    "        temp=predict(data,tree)\n",
    "        return(temp)\n",
    "# getting the tree\n",
    "tree =decision_tree(data)\n",
    "#getting the predictions\n",
    "pred=[]\n",
    "for i in range(len(data)):\n",
    "    pred.append(predict(data.iloc[i],tree))\n",
    "#Checking the performance of ID3 algorithm\n",
    "print('The metrics for ID3 algorithm are::')\n",
    "predictions = pd.DataFrame(pred,columns=['predictions'])\n",
    "predictions['true'] = data['quality']\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predictions['true'],predictions['predictions']))\n",
    "#Computing the Decision Tree Classifier Using Scikitlearn\n",
    "print('The metrics for scikit learn implementation with a min_samples_split =10:: ')\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dr = DecisionTreeClassifier(min_samples_split=10)\n",
    "dr.fit(data.drop('quality',axis=1),data['quality'])\n",
    "pred_=dr.predict(data.drop('quality',axis=1))\n",
    "print(classification_report(predictions['true'],pred_))\n",
    "from sklearn.model_selection import cross_val_score\n",
    "l = (data.drop('quality',axis=1)).columns\n",
    "precision_=cross_val_score(dr,data[l],data['quality'],cv=3,scoring = 'precision_macro')\n",
    "recall_=cross_val_score(dr,data[l],data['quality'],cv=3,scoring = 'recall_macro')\n",
    "accuracy_=cross_val_score(dr,data[l],data['quality'],cv=3,scoring = 'accuracy')\n",
    "print('The metrics for a 3 fold cross validation on Decision tree classifier model')\n",
    "print('\\n')\n",
    "print('Macro precision:',sum(precision_)/3)\n",
    "print('Macro recall:',sum(recall_)/3)\n",
    "print('accuracy:',sum(accuracy_)/3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
